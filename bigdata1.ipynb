{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyML5GI+CMqWCOYmVIu1/s/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yadav-Raj-Ghimire/BigData/blob/main/bigdata1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Java (Spark needs Java to run)\n",
        "\n"
      ],
      "metadata": {
        "id": "60HTEfQIiROq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W87w4N36ai0d"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if Java is installed correctly"
      ],
      "metadata": {
        "id": "9lwL2afOil73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J21tgXXoazBL",
        "outputId": "2fcce89a-25c5-4ea8-fd95-3d568b72d3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"17.0.17\" 2025-10-21\n",
            "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import OS module to set environment variables and Tell the system where Java is installed"
      ],
      "metadata": {
        "id": "M0AWyo13ir2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "metadata": {
        "id": "X7F6pEYIa1X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tell the system where Java is installed # Download Apache Spark (big data processing engine) and # Extract (unzip) the Spark package"
      ],
      "metadata": {
        "id": "oSMU0DA-iwTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3wg7KmdbAlS",
        "outputId": "533d885c-68a0-43a3-a78f-8852bf241af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-04 02:58:30--  https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400446614 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.1-bin-hadoop3.tgz.2’\n",
            "\n",
            "spark-3.5.1-bin-had 100%[===================>] 381.90M  19.1MB/s    in 23s     \n",
            "\n",
            "2026-02-04 02:58:54 (16.7 MB/s) - ‘spark-3.5.1-bin-hadoop3.tgz.2’ saved [400446614/400446614]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tell the system where Spark is installed"
      ],
      "metadata": {
        "id": "8mrbaoUpjNAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "mrwrqwjMbCjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install findspark to help Python find Spark"
      ],
      "metadata": {
        "id": "PR0NthtajVc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "GMTMPTtgbE4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import findspark library # Initialize Spark so Python can use it"
      ],
      "metadata": {
        "id": "gCZS9uAEjYk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "HspEe7jlbNtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#import pyspark so python can use it, import pyspark library, create and start sparkcontext (turn spark on), display sparkcontext to confirm it is running"
      ],
      "metadata": {
        "id": "vcXTrZOyjjZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Use SparkContext.getOrCreate() to prevent the \"Cannot run multiple SparkContexts at once\" error.\n",
        "# This method will return an existing SparkContext if one is running, or create a new one if it doesn't exist.\n",
        "conf = SparkConf().setAppName(\"CSE817\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "APhSbGJjb4_H",
        "outputId": "36357b8c-ad67-4a7c-cd2f-d2bb64f5d9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=CSE817>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a9215edf10f8:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>CSE817</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Hadoop (used for big data storage support) and # Extract (unzip) the Hadoop package"
      ],
      "metadata": {
        "id": "EPrnTaZsj6ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar xf hadoop-3.3.6.tar.gz\n"
      ],
      "metadata": {
        "id": "Mb4b7AvZcCFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Hadoop version to confirm installation"
      ],
      "metadata": {
        "id": "ZgfAWMSmkCT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/hadoop-3.3.6/bin/hadoop version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuDWOKXwca3X",
        "outputId": "3878c878-9f29-45eb-f672-d5a99c9f7824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Welcome to BigData Class\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egifwJIOfUk5",
        "outputId": "e7182a42-27ca-47a3-d14b-8b46448093ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to BigData Class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Configure Spark\n",
        "conf = SparkConf().setAppName(\"pyspark\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Print a message to indicate the program has started\n",
        "print(\"STARTED=============\")\n",
        "\n",
        "\n",
        "# Create a list of strings containing the \"~\" separator\n",
        "listr = [\"A~B\", \"C~D\", \"E~F\"]\n",
        "\n",
        "# Print the original list\n",
        "print(\"\\n===== RAW LIST ======\")\n",
        "print(listr)\n",
        "\n",
        "# Convert the list into an RDD (Resilient Distributed Dataset)\n",
        "rddstr = sc.parallelize(listr)\n",
        "\n",
        "# Print the RDD contents\n",
        "print(\"\\n===== RDD LIST ======\")\n",
        "print(rddstr.collect())\n",
        "\n",
        "# Use flatMap() to split each string by the \"~\" separator\n",
        "# flatMap() applies the split operation to each element and flattens the results into a single RDD\n",
        "flatdata = rddstr.flatMap(lambda x: x.split(\"~\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muPCyj4ak6Dd",
        "outputId": "6c517a2a-8a1a-4bce-8fd2-4ced456b62a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTED=============\n",
            "\n",
            "===== RAW LIST ======\n",
            "['A~B', 'C~D', 'E~F']\n",
            "\n",
            "===== RDD LIST ======\n",
            "['A~B', 'C~D', 'E~F']\n"
          ]
        }
      ]
    }
  ]
}